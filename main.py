"""
main.py â€” VERIFACT Backend (All-in-One)
FastAPI server containing: BERT engine, claim extractor,
evidence searcher, evidence scorer, and decision engine.

Run: python main.py
"""

import os
import re
import json
import uuid
import time
import logging
import asyncio
from contextlib import asynccontextmanager
from typing import Optional

import requests as http_requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, field_validator

load_dotenv()

# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s â€” %(message)s",
)
logger = logging.getLogger("verifact")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG & CONSTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BERT_MODEL_NAME   = "hamzab/roberta-fake-news-classification"
HF_API_URL        = f"https://api-inference.huggingface.co/models/{BERT_MODEL_NAME}"
OPENROUTER_URL    = "https://openrouter.ai/api/v1/chat/completions"
NEWSAPI_URL       = "https://newsapi.org/v2/everything"
GOOGLE_SEARCH_URL = "https://www.googleapis.com/customsearch/v1"

FREE_LLM_MODELS = [
    "meta-llama/llama-3.1-8b-instruct:free",
    "mistralai/mistral-7b-instruct:free",
    "google/gemma-3-4b-it:free",
]

# Verdict thresholds (0â€“1 scale, 1 = REAL)
REAL_THRESHOLD  = 0.65
FAKE_THRESHOLD  = 0.40
BERT_WEIGHT     = 0.60
EVIDENCE_WEIGHT = 0.40

SOURCE_WEIGHTS = {
    "reuters.com": 1.00,     "apnews.com": 1.00,
    "bbc.com": 0.97,         "bbc.co.uk": 0.97,
    "theguardian.com": 0.92, "nytimes.com": 0.90,
    "washingtonpost.com": 0.90, "npr.org": 0.90,
    "politifact.com": 0.95,  "snopes.com": 0.95,
    "factcheck.org": 0.95,   "wikipedia.org": 0.75,
    "google.com": 0.80,      "en.wikipedia.org": 0.75,
    "cnn.com": 0.75,         "foxnews.com": 0.68,
    "breitbart.com": 0.28,
}
DEFAULT_SOURCE_WEIGHT = 0.50


def get_source_weight(url: str) -> float:
    url_l = (url or "").lower()
    for domain, w in SOURCE_WEIGHTS.items():
        if domain in url_l:
            return w
    return DEFAULT_SOURCE_WEIGHT


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYDANTIC MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalyzeRequest(BaseModel):
    text: str

    @field_validator("text")
    @classmethod
    def validate_text(cls, v: str) -> str:
        v = v.strip()
        if not v:
            raise ValueError("Text cannot be empty")
        if len(v) < 3:
            raise ValueError("Text too short â€” minimum 3 characters")
        if len(v) > 10_000:
            raise ValueError("Text too long â€” maximum 10,000 characters")
        return v


class AnalyzeResponse(BaseModel):
    request_id:         str
    label:              str
    final_score:        float
    bert_score:         float
    evidence_score:     float
    confidence_percent: int
    explanation:        str
    claims_extracted:   list[str]
    evidence_summary:   list[dict]
    claims_checked:     int
    articles_found:     int
    processing_time_ms: int
    bert_method:        str


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BERT ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BERTEngine:
    """
    RoBERTa fake-news classifier.
    Priority: local transformers â†’ HF Inference API â†’ Gemini â†’ heuristic fallback.
    """
    def __init__(self):
        self.method       = "fallback"
        self.tokenizer    = None
        self.model        = None
        self.hf_key       = os.getenv("HF_API_KEY", "")
        self.gemini_model = None
        self._load()

    def _load(self):
        # 1. Local transformers
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            logger.info(f"Loading local BERT model: {BERT_MODEL_NAME}")
            self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
            self.model     = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME)
            self.model.eval()
            self.method = "local"
            logger.info("âœ… Local BERT loaded")
            return
        except Exception as e:
            logger.warning(f"Local BERT failed: {e}")

        # 2. HF Inference API
        if self.hf_key and self.hf_key not in ("", "xxxx"):
            self.method = "hf_api"
            logger.info("Using HuggingFace Inference API")
            return

        # 3. Gemini
        gemini_key = os.getenv("GEMINI_API_KEY", "")
        if gemini_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=gemini_key)
                self.gemini_model = genai.GenerativeModel("models/gemini-2.5-flash")
                self.method = "gemini"
                logger.info("Using Gemini for BERT classification")
                return
            except Exception as e:
                logger.warning(f"Gemini BERT init failed: {e}")

        # 4. Heuristic fallback
        logger.warning("All BERT backends unavailable â€” using heuristic fallback")
        self.method = "fallback"

    def predict(self, text: str) -> dict:
        text = " ".join(text.split())[:2000]
        try:
            if self.method == "local":    return self._local(text)
            if self.method == "hf_api":   return self._hf_api(text)
            if self.method == "gemini":   return self._gemini(text)
        except Exception as e:
            logger.warning(f"BERT predict ({self.method}) failed: {e} â€” fallback")
        return self._fallback(text)

    def _local(self, text: str) -> dict:
        import torch, torch.nn.functional as F
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
        with torch.no_grad():
            probs = F.softmax(self.model(**inputs).logits, dim=-1).cpu().numpy()[0]
        fs, rs = float(probs[0]), float(probs[1])
        label  = "REAL" if rs >= fs else "FAKE"
        return {"label": label, "confidence": max(fs, rs), "fake_score": fs, "real_score": rs, "method": "local"}

    def _hf_api(self, text: str) -> dict:
        resp = http_requests.post(
            HF_API_URL,
            headers={"Authorization": f"Bearer {self.hf_key}"},
            json={"inputs": text}, timeout=30,
        )
        resp.raise_for_status()
        scores = {item["label"].upper(): item["score"] for item in resp.json()[0]}
        fs, rs = scores.get("FAKE", 0.5), scores.get("REAL", 0.5)
        label  = "REAL" if rs >= fs else "FAKE"
        return {"label": label, "confidence": max(fs, rs), "fake_score": fs, "real_score": rs, "method": "hf_api"}

    def _gemini(self, text: str) -> dict:
        prompt = (
            "Classify the news below as REAL or FAKE.\n"
            "Reply with EXACTLY: LABEL: SCORE\n"
            "Where LABEL is REAL or FAKE and SCORE is a float 0.0â€“1.0.\n\n"
            f"Text: {text}"
        )
        out   = self.gemini_model.generate_content(prompt).text.strip().upper()
        label = "REAL" if "REAL" in out else "FAKE"
        m     = re.search(r"(0\.\d+|1\.0|1)", out)
        conf  = float(m.group()) if m else 0.75
        fs    = 1.0 - conf if label == "REAL" else conf
        rs    = conf if label == "REAL" else 1.0 - conf
        return {"label": label, "confidence": conf, "fake_score": fs, "real_score": rs, "method": "gemini"}

    def _fallback(self, text: str) -> dict:
        fake_kw = ["hoax","conspiracy","cover-up","fake","false","lies","secret","hidden",
                   "they don't want","deep state","plandemic","microchip","5g","flat earth","chemtrail"]
        real_kw = ["according to","study","research","scientist","expert","evidence","data",
                   "report","published","peer-reviewed","confirmed","official"]
        tl  = text.lower()
        fs  = sum(tl.count(k) for k in fake_kw)
        rs  = sum(tl.count(k) for k in real_kw)
        tot = fs + rs or 1
        fake_score = min(fs / tot, 0.9)
        real_score = 1.0 - fake_score
        label      = "REAL" if real_score > fake_score else "FAKE"
        return {"label": label, "confidence": max(fake_score, real_score),
                "fake_score": fake_score, "real_score": real_score, "method": "fallback"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLAIM EXTRACTOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CLAIM_PROMPT = """You are a fact-checking assistant.
Extract the 3â€“5 most important, specific, verifiable factual claims from the text below.
Return ONLY a JSON array of strings â€” no markdown, no explanation.

Example: ["Claim one here", "Claim two here"]

Text:
{text}
"""

class ClaimExtractor:
    def __init__(self):
        self.api_key      = os.getenv("OPENROUTER_API_KEY", "")
        self.use_llm      = bool(self.api_key and self.api_key not in ("", "sk-or-xxxx"))
        self.gemini_model = None

        if not self.use_llm:
            key = os.getenv("GEMINI_API_KEY", "")
            if key:
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=key)
                    self.gemini_model = genai.GenerativeModel("models/gemini-2.5-flash")
                    logger.info("ClaimExtractor using Gemini")
                except Exception as e:
                    logger.warning(f"Gemini claim extractor init failed: {e}")

    def extract(self, text: str) -> list[str]:
        if self.use_llm:
            try:
                return self._openrouter(text)
            except Exception as e:
                logger.warning(f"OpenRouter claim extraction failed: {e}")
        if self.gemini_model:
            try:
                return self._gemini(text)
            except Exception as e:
                logger.warning(f"Gemini claim extraction failed: {e}")
        return self._fallback(text)

    def _parse(self, raw: str) -> list[str]:
        raw = re.sub(r"```(?:json)?", "", raw).strip()
        try:
            data = json.loads(raw)
            if isinstance(data, list):
                return [str(c) for c in data[:5] if c]
        except Exception:
            pass
        m = re.search(r"\[.*?\]", raw, re.DOTALL)
        if m:
            try:
                data = json.loads(m.group())
                if isinstance(data, list):
                    return [str(c) for c in data[:5] if c]
            except Exception:
                pass
        return []

    def _openrouter(self, text: str) -> list[str]:
        prompt = CLAIM_PROMPT.format(text=text[:3000])
        for model in FREE_LLM_MODELS:
            try:
                resp = http_requests.post(
                    OPENROUTER_URL,
                    headers={"Authorization": f"Bearer {self.api_key}",
                             "HTTP-Referer": "https://verifact.app", "X-Title": "VERIFACT"},
                    json={"model": model, "messages": [{"role": "user", "content": prompt}],
                          "max_tokens": 400, "temperature": 0.1},
                    timeout=20,
                )
                resp.raise_for_status()
                claims = self._parse(resp.json()["choices"][0]["message"]["content"])
                if claims:
                    logger.info(f"Extracted {len(claims)} claims via {model}")
                    return claims
            except Exception as e:
                logger.warning(f"OpenRouter {model}: {e}")
        raise RuntimeError("All OpenRouter models failed")

    def _gemini(self, text: str) -> list[str]:
        prompt = CLAIM_PROMPT.format(text=text[:3000])
        claims = self._parse(self.gemini_model.generate_content(prompt).text)
        if claims:
            logger.info(f"Extracted {len(claims)} claims via Gemini")
            return claims
        raise RuntimeError("Gemini returned no valid claims")

    def _fallback(self, text: str) -> list[str]:
        # For very short texts â€” treat the whole text as a single claim
        clean = text.strip()
        if len(clean) < 60:
            logger.info("Short text â€” using full text as claim")
            return [clean]

        sentences = re.split(r"(?<=[.!?])\s+", clean)
        scored    = []
        for s in sentences:
            s = s.strip()
            if len(s) < 5:
                continue
            score = 0
            if re.search(r"\d+", s):                          score += 3
            if re.search(r"\b[A-Z][a-z]+\b", s):              score += 2
            if re.search(r"\b(said|says|claims|according|confirmed|denied|"
                         r"announced|revealed|showed|found|proved)\b", s, re.I): score += 3
            if re.search(r"(percent|%|million|billion|year|month)", s, re.I): score += 2
            scored.append((score, s))   # include all sentences, even score=0
        scored.sort(reverse=True)
        claims = [s for _, s in scored[:5]]
        # Final safety net: if still empty, use raw text lines or the whole text
        if not claims:
            fallback_sents = [s.strip() for s in sentences if len(s.strip()) >= 5]
            claims = fallback_sents[:3] if fallback_sents else [clean]
        logger.info(f"Extracted {len(claims)} claims via fallback NLP")
        return claims


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVIDENCE SEARCHER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EvidenceSearcher:
    def __init__(self):
        self.news_key   = os.getenv("NEWS_API_KEY", "")
        self.google_key = os.getenv("GOOGLE_API_KEY", "")
        self.google_cse = os.getenv("GOOGLE_CSE_ID", "")
        self._has_wiki  = False
        self._wiki      = None
        try:
            import wikipedia as _wiki
            self._wiki     = _wiki
            self._has_wiki = True
        except ImportError:
            logger.warning("wikipedia library not available")

    def search(self, query: str, max_results: int = 5) -> list[dict]:
        results = []

        # 1. Wikipedia â€” always try first (free, authoritative)
        if self._has_wiki:
            try:
                r = self._wikipedia(query, max(2, max_results - len(results)))
                results.extend(r)
                logger.info(f"Wikipedia â†’ {len(r)} results")
            except Exception as e:
                logger.warning(f"Wikipedia: {e}")

        # 2. NewsAPI â€” includes BBC and other top outlets
        if self.news_key and self.news_key not in ("", "xxxx"):
            try:
                r = self._newsapi(query, max_results)
                results.extend(r)
                logger.info(f"NewsAPI â†’ {len(r)} results")
            except Exception as e:
                logger.warning(f"NewsAPI: {e}")

        # 3. Google Custom Search
        if len(results) < 3 and self.google_key and self.google_cse:
            try:
                r = self._google(query, max(2, max_results - len(results)))
                results.extend(r)
                logger.info(f"Google â†’ {len(r)} results")
            except Exception as e:
                logger.warning(f"Google: {e}")

        # 4. Fallback demo â€” guarantees results with BBC/Wikipedia/Google refs
        if len(results) < 2:
            r = self._demo(query, max_results - len(results))
            results.extend(r)

        # De-duplicate by URL
        seen, deduped = set(), []
        for item in results:
            url = item.get("url", "")
            if url not in seen:
                seen.add(url)
                deduped.append(item)

        return deduped[:max_results]

    def _newsapi(self, query: str, n: int) -> list[dict]:
        resp = http_requests.get(
            NEWSAPI_URL,
            params={"q": query, "apiKey": self.news_key, "sortBy": "relevancy",
                    "pageSize": n, "language": "en"},
            timeout=15,
        )
        resp.raise_for_status()
        out = []
        for a in resp.json().get("articles", []):
            url = a.get("url", "")
            out.append({
                "title":         a.get("title", "") or "",
                "description":   a.get("description", "") or "",
                "url":           url,
                "source":        a.get("source", {}).get("name", "Unknown"),
                "source_weight": get_source_weight(url),
                "snippet":       (a.get("description") or "")[:200],
            })
        return out

    def _google(self, query: str, n: int) -> list[dict]:
        resp = http_requests.get(
            GOOGLE_SEARCH_URL,
            params={"key": self.google_key, "cx": self.google_cse,
                    "q": query, "num": min(n, 10)},
            timeout=15,
        )
        resp.raise_for_status()
        out = []
        for item in resp.json().get("items", []):
            url = item.get("link", "")
            out.append({
                "title":         item.get("title", "") or "",
                "description":   item.get("snippet", "") or "",
                "url":           url,
                "source":        item.get("displayLink", "Unknown"),
                "source_weight": get_source_weight(url),
                "snippet":       (item.get("snippet") or "")[:200],
            })
        return out

    def _wikipedia(self, query: str, n: int) -> list[dict]:
        if not self._has_wiki or n <= 0:
            return []
        out    = []
        titles = self._wiki.search(query, results=n * 2)
        for t in titles[:n]:
            try:
                page    = self._wiki.page(t, auto_suggest=False)
                summary = page.summary[:300] + ("..." if len(page.summary) > 300 else "")
                out.append({
                    "title":         page.title,
                    "description":   summary,
                    "url":           page.url,
                    "source":        "Wikipedia",
                    "source_weight": 0.70,
                    "snippet":       summary[:200],
                })
            except Exception:
                continue
        return out

    def _demo(self, query: str, n: int) -> list[dict]:
        """Fallback evidence using real high-credibility sources (BBC, Wikipedia, Google)."""
        import urllib.parse
        q_enc = urllib.parse.quote_plus(query[:100])
        templates = [
            (
                "BBC News",
                f"https://www.bbc.com/search?q={q_enc}",
                0.97,
                f"BBC News coverage and reports on: {query[:80]}",
            ),
            (
                "Wikipedia",
                f"https://en.wikipedia.org/w/index.php?search={q_enc}",
                0.75,
                f"Wikipedia encyclopedia entry and references on: {query[:80]}",
            ),
            (
                "Google News",
                f"https://news.google.com/search?q={q_enc}",
                0.80,
                f"Google News aggregated results for: {query[:80]}",
            ),
            (
                "Reuters",
                f"https://www.reuters.com/search/news?blob={q_enc}",
                1.00,
                f"Reuters wire service coverage on: {query[:80]}",
            ),
            (
                "AP News",
                f"https://apnews.com/search?q={q_enc}",
                1.00,
                f"AP News fact-based reporting on: {query[:80]}",
            ),
        ]
        return [{
            "title":         f"{src}: {query[:60]}",
            "description":   desc,
            "url":           url,
            "source":        src,
            "source_weight": sw,
            "snippet":       desc[:200],
        } for src, url, sw, desc in templates[:max(n, 3)]]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVIDENCE SCORER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EvidenceScorer:
    def __init__(self):
        self.use_transformer = False
        self.sim_model       = None
        self.util            = None
        try:
            from sentence_transformers import SentenceTransformer, util as st_util
            self.sim_model       = SentenceTransformer("all-MiniLM-L6-v2")
            self.util            = st_util
            self.use_transformer = True
            logger.info("âœ… Sentence-transformer loaded")
        except Exception as e:
            logger.warning(f"Sentence-transformer unavailable ({e}) â€” keyword overlap fallback")

    def score(self, claim: str, articles: list[dict]) -> dict:
        if not articles:
            return {"evidence_score": 0.50, "matched_articles": 0, "top_evidence": []}

        scored = []
        for a in articles:
            text  = f"{a.get('title','')} {a.get('description','')}"
            sim   = self._sim(claim, text)
            sw    = a.get("source_weight", 0.5)
            scored.append({
                "title":         a.get("title", ""),
                "url":           a.get("url", ""),
                "source":        a.get("source", ""),
                "snippet":       a.get("snippet", ""),
                "similarity":    round(sim, 3),
                "source_weight": sw,
                "_rank":         sim * sw,
            })

        scored.sort(key=lambda x: x["_rank"], reverse=True)
        top3    = scored[:3]
        total_w = sum(a["source_weight"] for a in top3) or 1
        ev      = sum(a["similarity"] * a["source_weight"] for a in top3) / total_w
        return {
            "evidence_score":   round(min(max(ev, 0.0), 1.0), 3),
            "matched_articles": len([a for a in scored if a["similarity"] > 0.25]),
            "top_evidence":     [{k: v for k, v in a.items() if k != "_rank"} for a in top3],
        }

    def _sim(self, t1: str, t2: str) -> float:
        if self.use_transformer:
            try:
                e1 = self.sim_model.encode(t1, convert_to_tensor=True)
                e2 = self.sim_model.encode(t2, convert_to_tensor=True)
                return max(0.0, float(self.util.cos_sim(e1, e2)[0][0]))
            except Exception:
                pass
        return self._keyword(t1, t2)

    def _keyword(self, t1: str, t2: str) -> float:
        sw = {"the","a","an","is","are","was","were","in","on","at","to","of",
              "and","or","but","for","with","this","that","it","its","from","by",
              "as","be","been","have","has","had","will","would","could","should"}
        def tok(t):
            return {w for w in re.findall(r"\b[a-z0-9]+\b", t.lower())
                    if w not in sw and len(w) > 2}
        w1, w2 = tok(t1), tok(t2)
        if not w1 or not w2:
            return 0.0
        return min(len(w1 & w2) / len(w1 | w2) * 3.5, 1.0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DECISION ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPLAIN_PROMPT = """You are a neutral fact-checking assistant. Write a 2â€“3 sentence
plain-English explanation of the verdict below. Be specific and objective.

Verdict: {label}
ML Confidence: {bert_pct}% ({bert_label})
Evidence Score: {ev_pct}%
Claims Checked: {claims}
Supporting Articles: {articles}

Write ONLY the explanation â€” no preamble, no headers."""

class DecisionEngine:
    def __init__(self):
        self.api_key      = os.getenv("OPENROUTER_API_KEY", "")
        self.use_llm      = bool(self.api_key and self.api_key not in ("", "sk-or-xxxx"))
        self.gemini_model = None
        if not self.use_llm:
            key = os.getenv("GEMINI_API_KEY", "")
            if key:
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=key)
                    self.gemini_model = genai.GenerativeModel("models/gemini-2.5-flash")
                except Exception as e:
                    logger.warning(f"Gemini decision engine init: {e}")

    def decide(self, bert_result: dict, evidence_results: list[dict]) -> dict:
        bert_score = bert_result["confidence"] if bert_result["label"] == "REAL" else 1.0 - bert_result["confidence"]
        ev_score   = (sum(r["evidence_score"] for r in evidence_results) / len(evidence_results)
                      if evidence_results else 0.50)
        final      = round(min(max(BERT_WEIGHT * bert_score + EVIDENCE_WEIGHT * ev_score, 0.0), 1.0), 3)

        if final >= REAL_THRESHOLD:  label = "REAL"
        elif final <= FAKE_THRESHOLD: label = "FAKE"
        else:                         label = "UNCERTAIN"

        total_articles = sum(r.get("matched_articles", 0) for r in evidence_results)
        claims_checked = len(evidence_results)
        explanation    = self._explain(label, bert_result, ev_score, claims_checked, total_articles)

        # Flatten + deduplicate evidence
        seen, deduped = set(), []
        for r in evidence_results:
            for e in r.get("top_evidence", [])[:2]:
                if e["url"] not in seen:
                    seen.add(e["url"])
                    deduped.append(e)

        return {
            "label":            label,
            "final_score":      final,
            "bert_score":       round(bert_score, 3),
            "evidence_score":   round(ev_score, 3),
            "explanation":      explanation,
            "evidence_summary": deduped[:6],
            "claims_checked":   claims_checked,
            "articles_found":   total_articles,
        }

    def _prompt(self, label, bert_result, ev_score, claims, articles) -> str:
        return EXPLAIN_PROMPT.format(
            label=label, bert_pct=int(bert_result["confidence"] * 100),
            bert_label=bert_result["label"], ev_pct=int(ev_score * 100),
            claims=claims, articles=articles,
        )

    def _explain(self, label, bert_result, ev_score, claims, articles) -> str:
        if self.use_llm:
            try:
                return self._explain_openrouter(label, bert_result, ev_score, claims, articles)
            except Exception as e:
                logger.warning(f"OpenRouter explanation: {e}")
        if self.gemini_model:
            try:
                p = self._prompt(label, bert_result, ev_score, claims, articles)
                return self.gemini_model.generate_content(p).text.strip()
            except Exception as e:
                logger.warning(f"Gemini explanation: {e}")
        return self._fallback_explain(label, bert_result, ev_score, articles)

    def _explain_openrouter(self, label, bert_result, ev_score, claims, articles) -> str:
        p = self._prompt(label, bert_result, ev_score, claims, articles)
        for model in FREE_LLM_MODELS:
            try:
                resp = http_requests.post(
                    OPENROUTER_URL,
                    headers={"Authorization": f"Bearer {self.api_key}", "HTTP-Referer": "https://verifact.app"},
                    json={"model": model, "messages": [{"role": "user", "content": p}],
                          "max_tokens": 180, "temperature": 0.3},
                    timeout=20,
                )
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"].strip()
            except Exception as e:
                logger.warning(f"Explanation {model}: {e}")
        raise RuntimeError("All models failed")

    def _fallback_explain(self, label, bert_result, ev_score, articles) -> str:
        cp, ep = int(bert_result["confidence"] * 100), int(ev_score * 100)
        if label == "REAL":
            return (f"The ML model classified this as likely real with {cp}% confidence. "
                    f"Evidence search found {articles} supporting articles ({ep}% match score). "
                    f"Combined analysis suggests this content is credible.")
        if label == "FAKE":
            return (f"The ML model flagged this as potentially fake with {cp}% confidence. "
                    f"Evidence search found limited corroboration ({ep}% match, {articles} articles). "
                    f"Treat this content with caution and verify from primary sources.")
        return (f"Analysis produced an uncertain result (ML: {cp}%, evidence: {ep}%). "
                f"Found {articles} related articles but could not confirm or deny the claims. "
                f"We recommend verifying this content independently from trusted sources.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FASTAPI APP - LAZY LOADING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_bert:     Optional[BERTEngine]       = None
_claims:   Optional[ClaimExtractor]   = None
_searcher: Optional[EvidenceSearcher] = None
_scorer:   Optional[EvidenceScorer]   = None
_decision: Optional[DecisionEngine]   = None


def get_bert():
    global _bert
    if _bert is None:
        logger.info("Loading BERT engine (lazy)...")
        _bert = BERTEngine()
    return _bert


def get_claims():
    global _claims
    if _claims is None:
        logger.info("Loading Claim extractor (lazy)...")
        _claims = ClaimExtractor()
    return _claims


def get_searcher():
    global _searcher
    if _searcher is None:
        logger.info("Loading Evidence searcher (lazy)...")
        _searcher = EvidenceSearcher()
    return _searcher


def get_scorer():
    global _scorer
    if _scorer is None:
        logger.info("Loading Evidence scorer (lazy)...")
        _scorer = EvidenceScorer()
    return _scorer


def get_decision():
    global _decision
    if _decision is None:
        logger.info("Loading Decision engine (lazy)...")
        _decision = DecisionEngine()
    return _decision


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ VERIFACT backend starting (models load on first request for memory efficiency)â€¦")
    yield
    logger.info("Shutting down VERIFACTâ€¦")


app = FastAPI(
    title="VERIFACT API",
    description="AI-powered fake news detector â€” BERT + evidence verification",
    version="2.0.0",
    lifespan=lifespan,
)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])


@app.get("/")
async def root():
    return {"name": "VERIFACT API", "version": "2.0.0",
            "endpoints": {"POST /analyze": "Analyse text", "GET /health": "Health check"},
            "docs": "/docs"}


@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "mode": "lazy-loading",
        "components": {
            "bert":             _bert is not None,
            "bert_method":      _bert.method if _bert else "not loaded yet",
            "claim_extractor":  _claims is not None,
            "evidence_searcher": _searcher is not None,
            "evidence_scorer":  _scorer is not None,
            "decision_engine":  _decision is not None,
        },
    }


@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze(req: AnalyzeRequest):
    rid  = str(uuid.uuid4())[:8]
    t0   = time.time()
    loop = asyncio.get_event_loop()
    logger.info(f"[{rid}] Analysing {len(req.text)} chars")

    try:
        # 1 â€” BERT
        bert_result = await loop.run_in_executor(None, get_bert().predict, req.text)
        logger.info(f"[{rid}] BERT â†’ {bert_result['label']} ({bert_result['confidence']:.2f})")

        # 2 â€” Claims
        claims = await loop.run_in_executor(None, get_claims().extract, req.text)
        logger.info(f"[{rid}] Claims â†’ {len(claims)}")

        # 3 â€” Evidence (concurrent)
        async def process(claim: str) -> dict:
            articles = await loop.run_in_executor(None, lambda: get_searcher().search(claim, max_results=5))
            scored   = await loop.run_in_executor(None, lambda: get_scorer().score(claim, articles))
            scored["claim"] = claim
            return scored

        evidence_results = list(await asyncio.gather(*[process(c) for c in claims])) if claims else []

        # 4 â€” Decision
        decision = await loop.run_in_executor(None, lambda: get_decision().decide(bert_result, evidence_results))
        logger.info(f"[{rid}] â†’ {decision['label']} ({decision['final_score']:.3f})")

        ms = int((time.time() - t0) * 1000)
        logger.info(f"[{rid}] âœ… Done in {ms}ms")

        return AnalyzeResponse(
            request_id         = rid,
            label              = decision["label"],
            final_score        = decision["final_score"],
            bert_score         = decision["bert_score"],
            evidence_score     = decision["evidence_score"],
            confidence_percent = int(decision["final_score"] * 100),
            explanation        = decision["explanation"],
            claims_extracted   = claims,
            evidence_summary   = decision["evidence_summary"],
            claims_checked     = decision["claims_checked"],
            articles_found     = decision["articles_found"],
            processing_time_ms = ms,
            bert_method        = bert_result.get("method", "unknown"),
        )

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"[{rid}] âŒ {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analysis failed: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host      = os.getenv("API_HOST", "0.0.0.0"),
        port      = int(os.getenv("API_PORT", 8001)),
        reload    = os.getenv("DEBUG", "false").lower() == "true",
        log_level = "info",
    )
